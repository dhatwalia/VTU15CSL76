{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset,train_perc = 0.8):\n",
    "    data_len = len(dataset)\n",
    "    print(\"length of dataset\"+str(data_len))\n",
    "    train_index = int(data_len*train_perc)\n",
    "    \n",
    "    train = dataset[:train_index,:]\n",
    "    \n",
    "    test = dataset[train_index:,:]\n",
    "    \n",
    "    return(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(activation):\n",
    "    return 1.0/(1.0 + np.exp(-activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(prediction,actual):\n",
    "    return 0.5*np.sum((actual.T-prediction)*(actual.T-prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(train_X,W1,W2,layer1_output,layer2_output,actual_output):\n",
    "    difference = actual_output.T - layer2_output\n",
    "    delta_output = layer2_output*(1-layer2_output)*difference\n",
    "    delta_hidden = layer1_output*(1-layer1_output)*W2.T.dot(delta_output)\n",
    "    deltaW2 = lr*(delta_output.dot(layer1_output.T)/n_train)\n",
    "    deltaW1 = lr*(delta_hidden.dot(train_X)/n_train)\n",
    "    \n",
    "    return (deltaW1,deltaW2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(train_X,train_Y):\n",
    "    n_input = train_X.shape[1]\n",
    "    W1 = np.random.random((n_hidden,n_input))\n",
    "    W2 = np.random.random((num_classes,n_hidden))\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        layer1_output = sigmoid(W1.dot(train_X.T))\n",
    "        layer2_output = sigmoid(W2.dot(layer1_output))\n",
    "        \n",
    "        (deltaW1,deltaW2) = back_prop(train_X,W1,W2,layer1_output,layer2_output,train_Y)\n",
    "        \n",
    "        W2 = W2+deltaW2\n",
    "        W1 = W1+deltaW1\n",
    "        \n",
    "        if epoch%100 == 0:\n",
    "            loss = compute_loss(layer2_output,train_Y)\n",
    "            print(str.format('loss in {0}th epoch is {1}',epoch,loss))\n",
    "            \n",
    "    return (W1,W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_X,test_y,params):\n",
    "    (W1,W2) = params\n",
    "    layer1_output = sigmoid(W1.dot(test_X.T))\n",
    "    final = sigmoid(W2.dot(layer1_output))\n",
    "    \n",
    "    prediction = final.argmax(axis = 0)\n",
    "    return np.sum(prediction == test_y)/len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_OH(data,num_classes):\n",
    "    \n",
    "    one_hot = np.zeros((len(data),num_classes))  #ask here\n",
    "    \n",
    "    one_hot[np.arange(len(data)),data] = 1    #ask here\n",
    "    print(one_hot)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset210\n",
      "ntrain = 168\n",
      "ntest = 42\n",
      "no of classes:4\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "loss in 0th epoch is 251.98276530355645\n",
      "loss in 100th epoch is 251.98138884049501\n",
      "loss in 200th epoch is 251.97974256781876\n",
      "loss in 300th epoch is 251.97772926845502\n",
      "loss in 400th epoch is 251.97519453685572\n",
      "loss in 500th epoch is 251.97187507794936\n",
      "loss in 600th epoch is 251.96727669533564\n",
      "loss in 700th epoch is 251.96033580896423\n",
      "loss in 800th epoch is 251.94822172848367\n",
      "loss in 900th epoch is 251.91983806082663\n",
      "loss in 1000th epoch is 251.74037833365355\n",
      "loss in 1100th epoch is 223.39469043144365\n",
      "loss in 1200th epoch is 223.39134096918156\n",
      "loss in 1300th epoch is 223.38673041881063\n",
      "loss in 1400th epoch is 223.3799395573745\n",
      "loss in 1500th epoch is 223.3688000627615\n",
      "loss in 1600th epoch is 223.34644996392117\n",
      "loss in 1700th epoch is 223.2698215284251\n",
      "loss in 1800th epoch is 139.451078609367\n",
      "loss in 1900th epoch is 139.35664926613123\n",
      "Mean Accuracy: 83.333%\n"
     ]
    }
   ],
   "source": [
    "filename = 'train4.csv'\n",
    "\n",
    "df = pd.read_csv(filename,dtype = np.float64,header = None)\n",
    "dataset = np.array(df)\n",
    "\n",
    "(train,test) = split_dataset(dataset)\n",
    "n_train = len(train)\n",
    "print('ntrain = '+str(n_train))\n",
    "n_test = len(test)\n",
    "print('ntest = '+str(n_test))\n",
    "\n",
    "lr = 1\n",
    "n_epoch = 2000\n",
    "\n",
    "num_classes = len(np.unique(dataset[:,-1]))\n",
    "print('no of classes:'+str(num_classes))\n",
    "train_one_hot = convert_to_OH(train[:,-1].astype(int),num_classes)  #ask here\n",
    "\n",
    "n_hidden = 20\n",
    "\n",
    "params = train_network(train[:,:-1],train_one_hot)\n",
    "accuracy = evaluate(test[:,:-1],test[:,-1],params)*100\n",
    "print('Mean Accuracy: %.3f%%'%accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
